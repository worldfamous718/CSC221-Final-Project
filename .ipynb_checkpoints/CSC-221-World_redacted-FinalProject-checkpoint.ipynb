{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f6d98b-488f-4138-a849-9740d91208e8",
   "metadata": {},
   "source": [
    "![pAIthon Labs Logo](https://raw.githubusercontent.com/worldfamous718/pAIthon-Labs/main/Labs/Logos-Files/Lab-Logo.png)\n",
    "\n",
    "# Conclusion of CSC 221\n",
    "\n",
    "## Reflection on Learning Progress\n",
    "\n",
    "I am more than happy with what I have learned and the mindset for coding that I have developed. This project tested every bit of my patience, but my determination was the overall winner.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "For this project, I combined my basic knowledge of Python to develop a spoof. This spoof may just trick you into thinking the book you are buying is really something it isn't.\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "- Used a variety of different libraries and modules\n",
    "- Demonstrated the versatility of Python as a programming language\n",
    "- Learned new concepts and techniques along the way\n",
    "\n",
    "## Challenges Encountered\n",
    "\n",
    "Welcome to the life of a developer!\n",
    "\n",
    "Some previously known methods weren't working as expected. This led to further learning and problem-solving.\n",
    "\n",
    "## Project Status\n",
    "\n",
    "While it's not perfect yet, I'm proud of what I was able to figure out and make work. There's still room for improvement, and I still have some time to continue refining it.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "I'll continue working on optimizing and enhancing the project over the next 2 weeks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fccb5514-5f98-4535-82c2-1a80ae9c2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# World..redacted\n",
    "# CSC 221 Final Project\n",
    "# Web Scraping and Data Visualization\n",
    "# 11/21/2024\n",
    "\n",
    "# IMPORT LIBRARIES AND MODULES FOR MY PROJECT:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ca3da7-b357-414c-be1b-6fa38aa3277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD TIME DELAY BETWEEN FET REQUESTS TO AVOID ERRORS:\n",
    "\n",
    "time.sleep(2)  # Wait for 2 seconds between requests\n",
    "\n",
    "# THIS IS THE SITE I WILL BE SCRAPING\n",
    "BASE_URL = \"https://nostarch.com/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883d67f-1514-410b-9a82-84dde9f09610",
   "metadata": {},
   "source": [
    "# Building Our Web Scraper\n",
    "\n",
    "Before we dive into the code, let's take a moment to appreciate the power of web scraping!\n",
    "\n",
    "\n",
    "![Web Scraping](BSoup.jpg)\n",
    "\n",
    "In this project, we're going to build a web scraper to extract information about Python books from the No Starch Press website. We'll use the BeautifulSoup library to parse the HTML content and extract the relevant data.\n",
    "\n",
    "Our scraper will:\n",
    "- Scrape book titles, authors, and URLs\n",
    "- Generate fictional page counts and publication dates where real data isn't available\n",
    "- Save the extracted information to a CSV file\n",
    "- Display the first 10 results in the console\n",
    "\n",
    "This project showcases several key aspects of web scraping:\n",
    "- Using HTTP requests to fetch webpage content\n",
    "- Parsing HTML with BeautifulSoup\n",
    "- Handling dynamic content and missing data\n",
    "- Structuring and saving extracted information\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86dcf8b6-2dc7-4753-94e5-ec6105dac878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD WEB SCRAPER:\n",
    "\n",
    "def scrape_book_page(url):\n",
    "    full_url = f\"{BASE_URL}/{url}\"\n",
    "    response = requests.get(full_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    books = []\n",
    "\n",
    "    # I'M USING A REGULAR EXPRESSION HERE SO I CAN BROADEN MY SEARCH. IT WAS HARD TO FIND WHAT I WAS LOOKING FOR:\n",
    "    \n",
    "    book_elements = soup.find_all(['article', 'div'], class_=re.compile(r'node|product|teaser|clearfix'))\n",
    "\n",
    "    for element in book_elements:\n",
    "        h2_element = element.find('h2')\n",
    "        if h2_element and h2_element.a:\n",
    "            title = h2_element.a.text.strip()\n",
    "            url = BASE_URL + h2_element.a['href']\n",
    "\n",
    "            author_element = element.find('div', class_=re.compile(r'field-name-field-author|author'))\n",
    "            if author_element:\n",
    "                author_text = author_element.find('div', class_='field-items').find('div', class_='field-item even')\n",
    "                if author_text and author_text.text.strip():\n",
    "                    author = author_text.text.strip()\n",
    "\n",
    "            # ADD ITEMS TO MY BOOK LIST:\n",
    "            \n",
    "            books.append({\n",
    "                'Title': title,\n",
    "                'URL': url,\n",
    "                'Author': author if author else '',\n",
    "                'Number of Pages': '',\n",
    "                'Release Date': ''\n",
    "            })\n",
    "\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97227f47-7183-4d18-b6f2-1021634fe8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS WHY I WILL BE A DEVELOPER. ITS MY WORK AROUND FUNCTION TO FORCE ELEMENTS THAT I COULDNT GET TO SCRAPE\n",
    "# I AM USING RANDOM TO GENERATE INFO REGARDLESS IF THE SCRAPER MISSES SOMETHING. IT WAS MISSING PAGE  NUMBERS AND DATES:\n",
    "\n",
    "def scrape_book_details(books):\n",
    "    start_date = datetime(1999, 4, 1)\n",
    "    end_date = datetime(2024, 7, 2)\n",
    "\n",
    "    for book in books:\n",
    "        num_pages = str(random.randint(216, 855))\n",
    "        random_date = start_date + timedelta(\n",
    "            seconds=random.randint(0, int((end_date - start_date).total_seconds())),\n",
    "        )\n",
    "        release_date = random_date.strftime('%B %Y')\n",
    "\n",
    "        book['Number of Pages'] = f\"{num_pages} pp.\"\n",
    "        book['Release Date'] = release_date\n",
    "\n",
    "    return books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3edd73f-de58-439a-94ef-9f84aa645159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS HOW IM FORMATTING MY CSV FILE OUTPUT WITH ALL THE COLUMS I SCRAPED:\n",
    "\n",
    "def save_to_csv(books):\n",
    "    fieldnames = ['Title', 'Author', 'Number of Pages', 'Release Date', 'URL']\n",
    "    with open('python_books.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for index, book in enumerate(books, 1):\n",
    "            writer.writerow({\n",
    "                'Title': book.get('Title'),\n",
    "                'Author': book.get('Author', ''),\n",
    "                'Number of Pages': book.get('Number of Pages', '').strip(),\n",
    "                'Release Date': book.get('Release Date', '').strip(),\n",
    "                'URL': book.get('URL')\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f4afa9a-6078-4380-9aba-367d81e5365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS DIDNT LIKE MY CUSTOM WORKAROUND SO IT WAS GIVING ME ISSUES, THIS IS THE BEST I CAN COME UP WITH FOR OUTPUT:\n",
    "\n",
    "def display_first_10_results(books):\n",
    "    for index, book in enumerate(books[:10], 1):\n",
    "        print(f\"\\nBook {index}:\")\n",
    "        print(f\"Title: {book.get('Title', '')}\")\n",
    "        print(f\"Author: {book.get('Author', 'N')}\")\n",
    "        print(f\"Number of Pages: {book.get('Number of Pages', '')}\")\n",
    "        print(f\"Release Date: {book.get('Release Date', '')}\")\n",
    "        print(f\"URL: {book.get('URL')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923434ea-2cb1-4ca7-b3a8-878a708990f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete! Your spoofed .csv is ready!\n",
      "Here is a sample of your first 10 results:\n",
      "\n",
      "Book 1:\n",
      "Title: Python for Excel Power Users\n",
      "Author: Tracy Stephens\n",
      "Number of Pages: 633 pp.\n",
      "Release Date: September 2013\n",
      "URL: https://nostarch.com//python-excel\n",
      "\n",
      "Book 2:\n",
      "Title: Automate the Boring Stuff with Python, 3rd Edition\n",
      "Author: Al Sweigart\n",
      "Number of Pages: 634 pp.\n",
      "Release Date: August 2005\n",
      "URL: https://nostarch.com//automate-boring-stuff-python-3rd-edition\n",
      "\n",
      "Book 3:\n",
      "Title: Python Playground, 2nd Edition\n",
      "Author: Mahesh Venkitachalam\n",
      "Number of Pages: 759 pp.\n",
      "Release Date: January 2021\n",
      "URL: https://nostarch.com//python-playground-2nd-edition\n",
      "\n",
      "Book 4:\n",
      "Title: Python for Kids, 2nd Edition\n",
      "Author: Jason R. Briggs\n",
      "Number of Pages: 836 pp.\n",
      "Release Date: January 2008\n",
      "URL: https://nostarch.com//python-kids-2nd-edition\n",
      "\n",
      "Book 5:\n",
      "Title: Dive Into Data Science\n",
      "Author: Bradford Tuckfield\n",
      "Number of Pages: 329 pp.\n",
      "Release Date: May 2018\n",
      "URL: https://nostarch.com//dive-data-science\n",
      "\n",
      "Book 6:\n",
      "Title: Python Crash Course, 3rd Edition\n",
      "Author: Eric Matthes\n",
      "Number of Pages: 826 pp.\n",
      "Release Date: June 2018\n",
      "URL: https://nostarch.com//python-crash-course-3rd-edition\n",
      "\n",
      "Book 7:\n",
      "Title: Python Tools for Scientists\n",
      "Author: Lee Vaughan\n",
      "Number of Pages: 526 pp.\n",
      "Release Date: June 2001\n",
      "URL: https://nostarch.com//python-tools-scientists\n",
      "\n",
      "Book 8:\n",
      "Title: Python for Data Science\n",
      "Author: Yuli Vasiliev\n",
      "Number of Pages: 383 pp.\n",
      "Release Date: February 2009\n",
      "URL: https://nostarch.com//python-data-science\n",
      "\n",
      "Book 9:\n",
      "Title: The Art of Clean Code\n",
      "Author: Christian Mayer\n",
      "Number of Pages: 490 pp.\n",
      "Release Date: June 2017\n",
      "URL: https://nostarch.com//art-clean-code\n",
      "\n",
      "Book 10:\n",
      "Title: The Book of Dash\n",
      "Author: Adam Schroeder, Christian Mayer, and Ann Marie Ward\n",
      "Number of Pages: 431 pp.\n",
      "Release Date: November 2018\n",
      "URL: https://nostarch.com//book-dash\n"
     ]
    }
   ],
   "source": [
    "# THIS IS HOW SOMEONE WHO SAYS THEY WANT TO BE A DEVELOPER DEFINES A PROPER MAIN FUNCTION\n",
    "# AND TO THINK IN CSC 121 I DIDNT EVEN UNDERSTAND THE PURPOSE OF MAIN, LOL:\n",
    "\n",
    "def main():\n",
    "    url = \"catalog/python\"\n",
    "    books = scrape_book_page(url)\n",
    "    books = scrape_book_details(books)\n",
    "\n",
    "    save_to_csv(books)\n",
    "\n",
    "    print(\"Scraping complete! Your spoofed .csv is ready!\")\n",
    "    print('Here is a sample of your first 10 results:')\n",
    "    # Print only the first 10 results\n",
    "    #print(\"\\nFirst 10 Results:\")\n",
    "    display_first_10_results(books)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa4989-b7b1-44b4-8c46-88e3ebed7b28",
   "metadata": {},
   "source": [
    "# CSV Sample Output\n",
    "![CSV Sample Output](CSV-Output-Sample.png)\n",
    "\n",
    "### Notice the diffence?!\n",
    "\n",
    "# Python Books Catalog\r\n",
    "\r\n",
    "![Python Books Catalog](Python-by-No-Starch-Press.jpg)\r\n",
    "\r\n",
    "This catalog contains information about popular Python books available at No Starch Press.\r\n",
    "\r\n",
    "The data was scraped from their website using web scraping techniques and Python libraries such as `requests` and `BeautifulSoup`. \r\n",
    "\r\n",
    "Key features of this project:\r\n",
    "- Scrapes book titles, authors, page counts, and publication dates\r\n",
    "- Generates random page counts and dates when specific information wasn't available\r\n",
    "- Saves the data to a CSV file named `spoofed.csv`\r\n",
    "- Displays the first 10 results in tms of service.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42207c-3115-468e-831c-9c1dde9436b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
